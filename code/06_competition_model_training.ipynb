{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the competition model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly load all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_json_file_path = \"./datasets/competition_final/generated/final_year_1_to_10_data.json\"\n",
    "year_11_json_file_path = \"./datasets/competition_final/generated/final_year_11_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmID</th>\n",
       "      <th>year</th>\n",
       "      <th>playoff</th>\n",
       "      <th>averageWinRate</th>\n",
       "      <th>averagePoints</th>\n",
       "      <th>averageRebounds</th>\n",
       "      <th>averageAssists</th>\n",
       "      <th>averageSteals</th>\n",
       "      <th>averageBlocks</th>\n",
       "      <th>averageTurnovers</th>\n",
       "      <th>averageFGRatio</th>\n",
       "      <th>averageFTRatio</th>\n",
       "      <th>averageThreeRatio</th>\n",
       "      <th>coachWinRate</th>\n",
       "      <th>numberOfAwardedPlayers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WAS</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>244.629371</td>\n",
       "      <td>122.927739</td>\n",
       "      <td>47.384615</td>\n",
       "      <td>29.587413</td>\n",
       "      <td>12.310023</td>\n",
       "      <td>50.428904</td>\n",
       "      <td>0.418658</td>\n",
       "      <td>0.749509</td>\n",
       "      <td>0.280573</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TUL</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>196.262500</td>\n",
       "      <td>87.358333</td>\n",
       "      <td>39.875000</td>\n",
       "      <td>19.675000</td>\n",
       "      <td>8.566667</td>\n",
       "      <td>38.229167</td>\n",
       "      <td>0.414552</td>\n",
       "      <td>0.780216</td>\n",
       "      <td>0.327587</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SEA</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>274.577622</td>\n",
       "      <td>106.679720</td>\n",
       "      <td>55.923077</td>\n",
       "      <td>26.044755</td>\n",
       "      <td>11.078322</td>\n",
       "      <td>49.041958</td>\n",
       "      <td>0.412312</td>\n",
       "      <td>0.802031</td>\n",
       "      <td>0.310246</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SAS</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>318.134545</td>\n",
       "      <td>111.044848</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>34.910909</td>\n",
       "      <td>14.202424</td>\n",
       "      <td>50.506061</td>\n",
       "      <td>0.428546</td>\n",
       "      <td>0.808280</td>\n",
       "      <td>0.340362</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PHO</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>259.051748</td>\n",
       "      <td>113.709557</td>\n",
       "      <td>49.538462</td>\n",
       "      <td>21.234965</td>\n",
       "      <td>13.693240</td>\n",
       "      <td>47.233100</td>\n",
       "      <td>0.444417</td>\n",
       "      <td>0.801936</td>\n",
       "      <td>0.335072</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NYL</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>239.084416</td>\n",
       "      <td>93.432900</td>\n",
       "      <td>46.714286</td>\n",
       "      <td>19.831169</td>\n",
       "      <td>8.073593</td>\n",
       "      <td>40.969697</td>\n",
       "      <td>0.396254</td>\n",
       "      <td>0.808497</td>\n",
       "      <td>0.311745</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MIN</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>285.395041</td>\n",
       "      <td>127.131680</td>\n",
       "      <td>61.545455</td>\n",
       "      <td>30.919008</td>\n",
       "      <td>10.638567</td>\n",
       "      <td>50.823691</td>\n",
       "      <td>0.446331</td>\n",
       "      <td>0.781802</td>\n",
       "      <td>0.320708</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LAS</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>249.046281</td>\n",
       "      <td>122.257851</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>26.052893</td>\n",
       "      <td>14.456198</td>\n",
       "      <td>54.231405</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.807764</td>\n",
       "      <td>0.311575</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IND</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>284.894215</td>\n",
       "      <td>111.692011</td>\n",
       "      <td>51.090909</td>\n",
       "      <td>33.229752</td>\n",
       "      <td>16.182369</td>\n",
       "      <td>50.046832</td>\n",
       "      <td>0.409415</td>\n",
       "      <td>0.817711</td>\n",
       "      <td>0.328738</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CON</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>239.584848</td>\n",
       "      <td>93.972727</td>\n",
       "      <td>49.500000</td>\n",
       "      <td>22.596970</td>\n",
       "      <td>9.503030</td>\n",
       "      <td>45.674242</td>\n",
       "      <td>0.430382</td>\n",
       "      <td>0.791903</td>\n",
       "      <td>0.286320</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CHI</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>175.641958</td>\n",
       "      <td>83.803730</td>\n",
       "      <td>33.230769</td>\n",
       "      <td>16.469930</td>\n",
       "      <td>9.617249</td>\n",
       "      <td>36.235431</td>\n",
       "      <td>0.424166</td>\n",
       "      <td>0.757265</td>\n",
       "      <td>0.362627</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ATL</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>246.820979</td>\n",
       "      <td>110.401865</td>\n",
       "      <td>53.538462</td>\n",
       "      <td>27.850350</td>\n",
       "      <td>10.308625</td>\n",
       "      <td>47.002331</td>\n",
       "      <td>0.433360</td>\n",
       "      <td>0.752254</td>\n",
       "      <td>0.249799</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tmID  year  playoff  averageWinRate  averagePoints  averageRebounds  \\\n",
       "0   WAS    11      NaN        0.470588     244.629371       122.927739   \n",
       "1   TUL    11      NaN        0.500000     196.262500        87.358333   \n",
       "2   SEA    11      NaN        0.588235     274.577622       106.679720   \n",
       "3   SAS    11      NaN        0.441176     318.134545       111.044848   \n",
       "4   PHO    11      NaN        0.676471     259.051748       113.709557   \n",
       "5   NYL    11      NaN        0.382353     239.084416        93.432900   \n",
       "6   MIN    11      NaN        0.411765     285.395041       127.131680   \n",
       "7   LAS    11      NaN        0.529412     249.046281       122.257851   \n",
       "8   IND    11      NaN        0.647059     284.894215       111.692011   \n",
       "9   CON    11      NaN        0.470588     239.584848        93.972727   \n",
       "10  CHI    11      NaN        0.470588     175.641958        83.803730   \n",
       "11  ATL    11      NaN        0.529412     246.820979       110.401865   \n",
       "\n",
       "    averageAssists  averageSteals  averageBlocks  averageTurnovers  \\\n",
       "0        47.384615      29.587413      12.310023         50.428904   \n",
       "1        39.875000      19.675000       8.566667         38.229167   \n",
       "2        55.923077      26.044755      11.078322         49.041958   \n",
       "3        67.100000      34.910909      14.202424         50.506061   \n",
       "4        49.538462      21.234965      13.693240         47.233100   \n",
       "5        46.714286      19.831169       8.073593         40.969697   \n",
       "6        61.545455      30.919008      10.638567         50.823691   \n",
       "7        66.000000      26.052893      14.456198         54.231405   \n",
       "8        51.090909      33.229752      16.182369         50.046832   \n",
       "9        49.500000      22.596970       9.503030         45.674242   \n",
       "10       33.230769      16.469930       9.617249         36.235431   \n",
       "11       53.538462      27.850350      10.308625         47.002331   \n",
       "\n",
       "    averageFGRatio  averageFTRatio  averageThreeRatio  coachWinRate  \\\n",
       "0         0.418658        0.749509           0.280573      0.500000   \n",
       "1         0.414552        0.780216           0.327587      0.500000   \n",
       "2         0.412312        0.802031           0.310246      0.588235   \n",
       "3         0.428546        0.808280           0.340362      0.441176   \n",
       "4         0.444417        0.801936           0.335072      0.676471   \n",
       "5         0.396254        0.808497           0.311745      0.428571   \n",
       "6         0.446331        0.781802           0.320708      0.500000   \n",
       "7         0.420500        0.807764           0.311575      0.500000   \n",
       "8         0.409415        0.817711           0.328738      0.647059   \n",
       "9         0.430382        0.791903           0.286320      0.470588   \n",
       "10        0.424166        0.757265           0.362627      0.500000   \n",
       "11        0.433360        0.752254           0.249799      0.529412   \n",
       "\n",
       "    numberOfAwardedPlayers  \n",
       "0                        2  \n",
       "1                       10  \n",
       "2                        8  \n",
       "3                        2  \n",
       "4                        6  \n",
       "5                        2  \n",
       "6                        3  \n",
       "7                        6  \n",
       "8                        6  \n",
       "9                        1  \n",
       "10                       1  \n",
       "11                       3  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "original_json_file_path = \"./datasets/competition_final/generated/final_year_1_to_10_data.json\"\n",
    "year_11_json_file_path = \"./datasets/competition_final/generated/final_year_11_data.json\"\n",
    "\n",
    "df_original = pd.read_json(original_json_file_path)\n",
    "df_year_11 = pd.read_json(year_11_json_file_path)\n",
    "\n",
    "df_year_11.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model\n",
    "\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Combinations: 37it [00:16,  2.22it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mz:\\Faks\\Diplomski\\4. semestar (ERASMUS)\\Machine learning (ML - AC)\\project\\FEUP_ML_G79_basketball_playoffs\\code\\06_competition_model_training.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/z%3A/Faks/Diplomski/4.%20semestar%20%28ERASMUS%29/Machine%20learning%20%28ML%20-%20AC%29/project/FEUP_ML_G79_basketball_playoffs/code/06_competition_model_training.ipynb#W5sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mfor\u001b[39;00m params \u001b[39min\u001b[39;00m all_params:\n\u001b[0;32m     <a href='vscode-notebook-cell:/z%3A/Faks/Diplomski/4.%20semestar%20%28ERASMUS%29/Machine%20learning%20%28ML%20-%20AC%29/project/FEUP_ML_G79_basketball_playoffs/code/06_competition_model_training.ipynb#W5sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     model \u001b[39m=\u001b[39m LogisticRegression(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/z%3A/Faks/Diplomski/4.%20semestar%20%28ERASMUS%29/Machine%20learning%20%28ML%20-%20AC%29/project/FEUP_ML_G79_basketball_playoffs/code/06_competition_model_training.ipynb#W5sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     model\u001b[39m.\u001b[39;49mfit(X_train_subset, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/z%3A/Faks/Diplomski/4.%20semestar%20%28ERASMUS%29/Machine%20learning%20%28ML%20-%20AC%29/project/FEUP_ML_G79_basketball_playoffs/code/06_competition_model_training.ipynb#W5sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     accuracy_test \u001b[39m=\u001b[39m accuracy_score(y_test, model\u001b[39m.\u001b[39mpredict(X_test_subset))\n\u001b[0;32m     <a href='vscode-notebook-cell:/z%3A/Faks/Diplomski/4.%20semestar%20%28ERASMUS%29/Machine%20learning%20%28ML%20-%20AC%29/project/FEUP_ML_G79_basketball_playoffs/code/06_competition_model_training.ipynb#W5sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     accuracy_train \u001b[39m=\u001b[39m accuracy_score(y_train, model\u001b[39m.\u001b[39mpredict(X_train_subset))\n",
      "File \u001b[1;32mz:\\Programs\\Python3\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mz:\\Programs\\Python3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1303\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1301\u001b[0m     n_threads \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1303\u001b[0m fold_coefs_ \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, prefer\u001b[39m=\u001b[39;49mprefer)(\n\u001b[0;32m   1304\u001b[0m     path_func(\n\u001b[0;32m   1305\u001b[0m         X,\n\u001b[0;32m   1306\u001b[0m         y,\n\u001b[0;32m   1307\u001b[0m         pos_class\u001b[39m=\u001b[39;49mclass_,\n\u001b[0;32m   1308\u001b[0m         Cs\u001b[39m=\u001b[39;49m[C_],\n\u001b[0;32m   1309\u001b[0m         l1_ratio\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml1_ratio,\n\u001b[0;32m   1310\u001b[0m         fit_intercept\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[0;32m   1311\u001b[0m         tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[0;32m   1312\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m   1313\u001b[0m         solver\u001b[39m=\u001b[39;49msolver,\n\u001b[0;32m   1314\u001b[0m         multi_class\u001b[39m=\u001b[39;49mmulti_class,\n\u001b[0;32m   1315\u001b[0m         max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m   1316\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m   1317\u001b[0m         check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1318\u001b[0m         random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[0;32m   1319\u001b[0m         coef\u001b[39m=\u001b[39;49mwarm_start_coef_,\n\u001b[0;32m   1320\u001b[0m         penalty\u001b[39m=\u001b[39;49mpenalty,\n\u001b[0;32m   1321\u001b[0m         max_squared_sum\u001b[39m=\u001b[39;49mmax_squared_sum,\n\u001b[0;32m   1322\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1323\u001b[0m         n_threads\u001b[39m=\u001b[39;49mn_threads,\n\u001b[0;32m   1324\u001b[0m     )\n\u001b[0;32m   1325\u001b[0m     \u001b[39mfor\u001b[39;49;00m class_, warm_start_coef_ \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(classes_, warm_start_coef)\n\u001b[0;32m   1326\u001b[0m )\n\u001b[0;32m   1328\u001b[0m fold_coefs_, _, n_iter_ \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(n_iter_, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32)[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mz:\\Programs\\Python3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mz:\\Programs\\Python3\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mz:\\Programs\\Python3\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mz:\\Programs\\Python3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mz:\\Programs\\Python3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[0;32m    468\u001b[0m     l2_reg_strength \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m C\n\u001b[0;32m    469\u001b[0m     args \u001b[39m=\u001b[39m (X, target, sample_weight, l2_reg_strength, n_threads)\n\u001b[1;32m--> 470\u001b[0m     w0, n_iter_i \u001b[39m=\u001b[39m _newton_cg(\n\u001b[0;32m    471\u001b[0m         hess, func, grad, w0, args\u001b[39m=\u001b[39;49margs, maxiter\u001b[39m=\u001b[39;49mmax_iter, tol\u001b[39m=\u001b[39;49mtol\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m \u001b[39melif\u001b[39;00m solver \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnewton-cholesky\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    474\u001b[0m     \u001b[39m# The division by sw_sum is a consequence of the rescaling of\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     \u001b[39m# sample_weight, see comment above.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m     l2_reg_strength \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m C \u001b[39m/\u001b[39m sw_sum\n",
      "File \u001b[1;32mz:\\Programs\\Python3\\Lib\\site-packages\\sklearn\\utils\\optimize.py:182\u001b[0m, in \u001b[0;36m_newton_cg\u001b[1;34m(grad_hess, func, grad, x0, args, tol, maxiter, maxinner, line_search, warn)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39m# Outer loop: our Newton iteration\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[39mwhile\u001b[39;00m k \u001b[39m<\u001b[39m maxiter:\n\u001b[0;32m    180\u001b[0m     \u001b[39m# Compute a search direction pk by applying the CG method to\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[39m#  del2 f(xk) p = - fgrad f(xk) starting from 0.\u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m     fgrad, fhess_p \u001b[39m=\u001b[39m grad_hess(xk, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    184\u001b[0m     absgrad \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mabs(fgrad)\n\u001b[0;32m    185\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mmax(absgrad) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m tol:\n",
      "File \u001b[1;32mz:\\Programs\\Python3\\Lib\\site-packages\\sklearn\\linear_model\\_linear_loss.py:548\u001b[0m, in \u001b[0;36mLinearModelLoss.gradient_hessian_product\u001b[1;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads)\u001b[0m\n\u001b[0;32m    545\u001b[0m weights, intercept, raw_prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_intercept_raw(coef, X)\n\u001b[0;32m    547\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_loss\u001b[39m.\u001b[39mis_multiclass:\n\u001b[1;32m--> 548\u001b[0m     grad_pointwise, hess_pointwise \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_loss\u001b[39m.\u001b[39;49mgradient_hessian(\n\u001b[0;32m    549\u001b[0m         y_true\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    550\u001b[0m         raw_prediction\u001b[39m=\u001b[39;49mraw_prediction,\n\u001b[0;32m    551\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    552\u001b[0m         n_threads\u001b[39m=\u001b[39;49mn_threads,\n\u001b[0;32m    553\u001b[0m     )\n\u001b[0;32m    554\u001b[0m     grad \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty_like(coef, dtype\u001b[39m=\u001b[39mweights\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    555\u001b[0m     grad[:n_features] \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m grad_pointwise \u001b[39m+\u001b[39m l2_reg_strength \u001b[39m*\u001b[39m weights\n",
      "File \u001b[1;32mz:\\Programs\\Python3\\Lib\\site-packages\\sklearn\\_loss\\loss.py:366\u001b[0m, in \u001b[0;36mBaseLoss.gradient_hessian\u001b[1;34m(self, y_true, raw_prediction, sample_weight, gradient_out, hessian_out, n_threads)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[39mif\u001b[39;00m hessian_out\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m hessian_out\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    364\u001b[0m     hessian_out \u001b[39m=\u001b[39m hessian_out\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 366\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcloss\u001b[39m.\u001b[39;49mgradient_hessian(\n\u001b[0;32m    367\u001b[0m     y_true\u001b[39m=\u001b[39;49my_true,\n\u001b[0;32m    368\u001b[0m     raw_prediction\u001b[39m=\u001b[39;49mraw_prediction,\n\u001b[0;32m    369\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    370\u001b[0m     gradient_out\u001b[39m=\u001b[39;49mgradient_out,\n\u001b[0;32m    371\u001b[0m     hessian_out\u001b[39m=\u001b[39;49mhessian_out,\n\u001b[0;32m    372\u001b[0m     n_threads\u001b[39m=\u001b[39;49mn_threads,\n\u001b[0;32m    373\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model training...\n",
    "from itertools import combinations, product\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assume df is your DataFrame\n",
    "# Define features and target\n",
    "\n",
    "features = ['averageWinRate', 'averagePoints', 'averageRebounds', 'averageAssists',\n",
    "            'averageSteals', 'averageBlocks', 'averageTurnovers', 'averageFGRatio',\n",
    "            'averageFTRatio', 'averageThreeRatio', 'coachWinRate', 'numberOfAwardedPlayers']\n",
    "target = 'playoff'\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train = df_original[df_original[\"year\"] < 7][features + ['year']]\n",
    "X_test = df_original[df_original[\"year\"] >= 7][features + ['year']]\n",
    "y_train = df_original[df_original[\"year\"] < 7][\"playoff\"]\n",
    "y_test = df_original[df_original[\"year\"] >= 7][\"playoff\"]\n",
    "\n",
    "df_year_only_train = X_train[\"year\"]\n",
    "df_year_only_test = X_test[\"year\"]\n",
    "\n",
    "best_accuracy_sum = 0\n",
    "best_feature_combination = None\n",
    "best_hyperparameters = None\n",
    "\n",
    "# Define hyperparameters grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'tol': [1e-4, 1e-3, 1e-2],\n",
    "    'max_iter': [10000]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "all_params = [dict(zip(param_grid, v)) for v in product(*param_grid.values())]\n",
    "\n",
    "# Iterate through all possible feature combinations with 2 to 6 features\n",
    "results = []\n",
    "for subset in tqdm(combinations(features, 4), desc=\"Feature Combinations\"):\n",
    "    selected_features = list(subset) + ['year']\n",
    "\n",
    "    X_train_subset = X_train[selected_features]\n",
    "    X_test_subset = X_test[selected_features]\n",
    "\n",
    "    for params in all_params:\n",
    "        model = LogisticRegression(**params)\n",
    "        model.fit(X_train_subset, y_train)\n",
    "\n",
    "        accuracy_test = accuracy_score(y_test, model.predict(X_test_subset))\n",
    "        accuracy_train = accuracy_score(y_train, model.predict(X_train_subset))\n",
    "\n",
    "        if accuracy_test > 0.75:\n",
    "            results.append({\n",
    "                'acc_train': accuracy_train,\n",
    "                'acc_test': accuracy_test,\n",
    "                'best_feature_combination': str(selected_features),\n",
    "                'best_hyperparameters': str(params)\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "df_cd_json = results_df.to_json(orient=\"records\")\n",
    "new_file = open(\"./datasets/generated/logreg_hyper_param_tuning_2.json\", \"w\")\n",
    "new_file.writelines(df_cd_json)\n",
    "new_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training...\n",
    "from itertools import combinations, product\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Assume df is your DataFrame\n",
    "# Define features and target\n",
    "features = ['averageWinRate', 'averagePoints', 'averageRebounds', 'averageAssists',\n",
    "            'averageSteals', 'averageBlocks', 'averageTurnovers', 'averageFGRatio',\n",
    "            'averageFTRatio', 'averageThreeRatio', 'coachWinRate', 'numberOfAwardedPlayers']\n",
    "target = 'playoff'\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train = df_original[df_original[\"year\"] < 7][features + ['year']]\n",
    "X_test = df_original[df_original[\"year\"] >= 7][features + ['year']]\n",
    "y_train = df_original[df_original[\"year\"] < 7][\"playoff\"]\n",
    "y_test = df_original[df_original[\"year\"] >= 7][\"playoff\"]\n",
    "\n",
    "df_year_only_train = X_train[\"year\"]\n",
    "df_year_only_test = X_test[\"year\"]\n",
    "\n",
    "best_accuracy_sum = 0\n",
    "best_feature_combination = None\n",
    "best_hyperparameters = None\n",
    "\n",
    "# Define hyperparameters grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],        # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf'],  # Kernel type\n",
    "    'gamma': ['scale', 'auto'],   # Kernel coefficient\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "all_params = [dict(zip(param_grid, v)) for v in product(*param_grid.values())]\n",
    "\n",
    "# Iterate through all possible feature combinations with 2 to 6 features\n",
    "results = pd.DataFrame(columns=['acc_train', 'acc_test', 'best_feature_combination', 'best_hyperparameters'])\n",
    "\n",
    "# Always include 'year\n",
    "\n",
    "X_train = pd.concat([X_train, df_year_only_train], axis=1)\n",
    "X_test = pd.concat([X_test, df_year_only_test], axis=1)\n",
    "\n",
    "svm = SVC(kernel='linear')  # You can change the kernel as needed (e.g., 'rbf' for radial basis function)\n",
    "\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "    \n",
    "results.head()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_train</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>best_feature_combination</th>\n",
       "      <th>best_hyperparameters</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5663</th>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>['averageAssists', 'averageTurnovers', 'number...</td>\n",
       "      <td>{'n_neighbors': 7, 'weights': 'uniform', 'metr...</td>\n",
       "      <td>0.046296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5664</th>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>['averageAssists', 'averageTurnovers', 'number...</td>\n",
       "      <td>{'n_neighbors': 7, 'weights': 'uniform', 'metr...</td>\n",
       "      <td>0.046296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22419</th>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>['averageAssists', 'averageTurnovers', 'averag...</td>\n",
       "      <td>{'n_neighbors': 7, 'weights': 'uniform', 'metr...</td>\n",
       "      <td>0.046296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22394</th>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>['averageAssists', 'averageTurnovers', 'averag...</td>\n",
       "      <td>{'n_neighbors': 7, 'weights': 'uniform', 'metr...</td>\n",
       "      <td>0.046296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22393</th>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>['averageAssists', 'averageTurnovers', 'averag...</td>\n",
       "      <td>{'n_neighbors': 7, 'weights': 'uniform', 'metr...</td>\n",
       "      <td>0.046296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       acc_train  acc_test                           best_feature_combination  \\\n",
       "5663    0.805556  0.759259  ['averageAssists', 'averageTurnovers', 'number...   \n",
       "5664    0.805556  0.759259  ['averageAssists', 'averageTurnovers', 'number...   \n",
       "22419   0.805556  0.759259  ['averageAssists', 'averageTurnovers', 'averag...   \n",
       "22394   0.805556  0.759259  ['averageAssists', 'averageTurnovers', 'averag...   \n",
       "22393   0.805556  0.759259  ['averageAssists', 'averageTurnovers', 'averag...   \n",
       "\n",
       "                                    best_hyperparameters      diff  \n",
       "5663   {'n_neighbors': 7, 'weights': 'uniform', 'metr...  0.046296  \n",
       "5664   {'n_neighbors': 7, 'weights': 'uniform', 'metr...  0.046296  \n",
       "22419  {'n_neighbors': 7, 'weights': 'uniform', 'metr...  0.046296  \n",
       "22394  {'n_neighbors': 7, 'weights': 'uniform', 'metr...  0.046296  \n",
       "22393  {'n_neighbors': 7, 'weights': 'uniform', 'metr...  0.046296  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = pd.read_json(\"./datasets/generated/model_evaluation_by_accuracy_knn.json\")\n",
    "\n",
    "file[\"diff\"] = abs(file[\"acc_train\"] - file[\"acc_test\"])\n",
    "\n",
    "df2 = file[(file[\"acc_test\"] > 0.75) & (file[\"diff\"] < 0.05)].sort_values(\"acc_test\", axis=0, ascending=False)\n",
    "\n",
    "df2.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average accuracy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7592592593"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"acc_test\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logreg evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_split</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.673913</td>\n",
       "      <td>0.013587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.087275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.050926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.094186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.097643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  year_split  acc_test  acc_train      diff\n",
       "0          5  0.687500   0.673913  0.013587\n",
       "0          6  0.731343   0.644068  0.087275\n",
       "0          7  0.759259   0.708333  0.050926\n",
       "0          8  0.650000   0.744186  0.094186\n",
       "0          9  0.629630   0.727273  0.097643"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load libraries\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "data_file_path = \"./datasets/competition_final/generated/final_year_1_to_10_data.json\"\n",
    "df = pd.read_json(data_file_path)\n",
    "df.head()\n",
    "\n",
    "features = ['averageWinRate', 'averageRebounds', 'averageBlocks', 'averageThreeRatio', 'coachWinRate', 'numberOfAwardedPlayers', 'year']\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"year_split\", \"acc_test\", \"acc_train\", \"diff\"])\n",
    "\n",
    "for i in range(5,11):\n",
    "  #Split dataset in such way that we use latest years for training and older years for testing\n",
    "  X_train=df[df[\"year\"]<i][features]\n",
    "  X_test=df[df[\"year\"]>=i][features]\n",
    "  y_train=df[df[\"year\"]<i][\"playoff\"]\n",
    "  y_test=df[df[\"year\"]>=i][\"playoff\"]\n",
    "\n",
    "  models = {}\n",
    "\n",
    "  #Logistic Regression\n",
    "  models['Logistic Regression'] = LogisticRegression(C=1, penalty= 'l2', solver='newton-cg', tol=0.001, max_iter=10000)\n",
    "\n",
    "  accuracy, precision, recall = {}, {}, {}\n",
    "\n",
    "  # Fit the classifier\n",
    "  models['Logistic Regression'].fit(X_train, y_train)\n",
    "\n",
    "  # Make predictions\n",
    "  y_pred = models['Logistic Regression'].predict(X_test)\n",
    "\n",
    "  y_pred_test = models['Logistic Regression'].predict(X_train)\n",
    "\n",
    "  temp_df = pd.DataFrame([[i, metrics.accuracy_score(y_test, y_pred), metrics.accuracy_score(y_train, y_pred_test), abs(metrics.accuracy_score(y_test, y_pred) - metrics.accuracy_score(y_train, y_pred_test))]], columns=[\"year_split\", \"acc_test\", \"acc_train\", \"diff\"])\n",
    "  \n",
    "  results_df = pd.concat([results_df, temp_df])\n",
    "\n",
    "  \n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6660322851554195"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[\"acc_test\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Logreg is better because it is more consistent.\n",
    "\n",
    "Best is year 7 split.\n",
    "\n",
    "After year 8 for some reason in all cases the precision and/or difference in betweeen test and train accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WAS</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TUL</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SEA</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SAS</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PHO</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NYL</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MIN</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LAS</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IND</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CON</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CHI</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ATL</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tmID label\n",
       "0   WAS     Y\n",
       "1   TUL     Y\n",
       "2   SEA     Y\n",
       "3   SAS     Y\n",
       "4   PHO     Y\n",
       "5   NYL     Y\n",
       "6   MIN     Y\n",
       "7   LAS     Y\n",
       "8   IND     Y\n",
       "9   CON     Y\n",
       "10  CHI     N\n",
       "11  ATL     Y"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "y1_10_data_path = \"./datasets/competition_final/generated/final_year_1_to_10_data.json\"\n",
    "y11_data_path = \"./datasets/competition_final/generated/final_year_11_data.json\"\n",
    "\n",
    "df_y1_10 = pd.read_json(y1_10_data_path)\n",
    "df_y11 = pd.read_json(y11_data_path)\n",
    "\n",
    "features = ['averageWinRate', 'averageRebounds', 'averageBlocks', 'averageThreeRatio', 'coachWinRate', 'numberOfAwardedPlayers', 'year']\n",
    "target = 'playoff'\n",
    "\n",
    "X_train = df_y1_10[df_y1_10[\"year\"] < 7][features]\n",
    "X_test = df_y1_10[df_y1_10[\"year\"] >= 7][features]\n",
    "y_train = df_y1_10[df_y1_10[\"year\"] < 7][target]\n",
    "y_test = df_y1_10[df_y1_10[\"year\"] >= 7][target]\n",
    "\n",
    "model = LogisticRegression(C=0.1, penalty= 'l2', solver= 'newton-cg', tol= 0.001, max_iter= 10000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_train=model.predict(X_train)\n",
    "\n",
    "pred=model.predict(df_y11[features])\n",
    "\n",
    "# Step 1: Convert predictions to a DataFrame\n",
    "pred_df = pd.DataFrame(pred, columns=['label'])\n",
    "\n",
    "# Step 2: Subset the original DataFrame to get the corresponding 'tmID' values\n",
    "# for the test set\n",
    "test_teams = df_y11[df_y11[\"year\"] >= 7][['tmID']].reset_index(drop=True)\n",
    "\n",
    "# Step 3: Join the DataFrames\n",
    "result_df = pd.concat([test_teams, pred_df], axis=1)\n",
    "\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
